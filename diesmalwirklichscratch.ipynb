{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array(()) # empty numpy array\n",
    "for filename in os.listdir('Dataset_tokenized_BPE'): # iterate through all files in the folder\n",
    "    if filename.endswith('.npy'): # if the file is a .npy file\n",
    "        for i in range(len(np.load(('Dataset_tokenized_BPE/' + filename), allow_pickle=True)[()].get('tokens'))): # iterate through all the tokens in the file\n",
    "            tokens = np.append(tokens, np.load(('Dataset_tokenized_BPE/' + filename), allow_pickle=True)[()].get('tokens')[i]) # append the tokens to the numpy array\n",
    "        continue \n",
    "    else: \n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smallest token:1.0, biggest token:499.0, number of tokens:(493,), missing tokens:{0, 176, 178, 180, 182, 89, 186}\n"
     ]
    }
   ],
   "source": [
    "print(\"smallest token:{}, biggest token:{}, number of tokens:{}, missing tokens:{}\".format(np.min(tokens),np.max(tokens),np.unique(tokens).shape,set(range(0, 500))-set(tokens))) # print some information about the tokens\n",
    "vocab_size = int(np.max(tokens))\n",
    "data = tf.cast(tokens, tf.int32) # put tokens into a tensor and cast to int32\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n] # first 90% of data\n",
    "val_data = data[n:] # last 10% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
       " array([[225, 332, 317,  49, 470, 277,  48, 387,  46, 316,  52, 480],\n",
       "        [204,  57, 411,  58, 226, 208,  57, 317,  65, 254, 214,  56]])>,\n",
       " <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
       " array([[332, 317,  49, 470, 277,  48, 387,  46, 316,  52, 480,  53],\n",
       "        [ 57, 411,  58, 226, 208,  57, 317,  65, 254, 214,  56, 238]])>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLOCK_SIZE = 12\n",
    "BATCH_SIZE = 2\n",
    "    \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = tf.random.uniform((BATCH_SIZE,), maxval=len(data)-BLOCK_SIZE, dtype=tf.int32)\n",
    "    inputs = tf.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
    "    targets = tf.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    return inputs, targets\n",
    "\n",
    "get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[187,  52, 225, 189,  55, 246, 190,  58, 246, 191,  69, 225],\n",
      "       [ 58, 463,  55, 369,  52, 238, 190,  53, 324,  53, 238, 198]])>, <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[ 52, 225, 189,  55, 246, 190,  58, 246, 191,  69, 225, 193],\n",
      "       [463,  55, 369,  52, 238, 190,  53, 324,  53, 238, 198,  60]])>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 12, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moin = get_batch('train') # tuple of two tensors (inputs, targets) of shape (batch_size, block_size)\n",
    "print(moin)\n",
    "inputs, targets = moin # unpack tuple\n",
    "\n",
    "layer = tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                          output_dim=512, # each token gets a 512-vector embedding\n",
    "                          input_length=BLOCK_SIZE)\n",
    "layer(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 512), dtype=float32, numpy=\n",
       "array([[-0.04916601, -0.02973673, -0.01249365, ..., -0.00418216,\n",
       "         0.0386625 ,  0.02667138],\n",
       "       [-0.03071208,  0.01060237,  0.02099904, ...,  0.03319449,\n",
       "        -0.04411687,  0.04185689],\n",
       "       [-0.03068695,  0.00648274,  0.03935588, ...,  0.03560025,\n",
       "         0.02234754,  0.0326318 ],\n",
       "       ...,\n",
       "       [-0.0050758 ,  0.04384517,  0.01985851, ..., -0.04682778,\n",
       "        -0.02236791,  0.03188093],\n",
       "       [ 0.00323981,  0.03662967, -0.04555812, ..., -0.04969875,\n",
       "        -0.04542772,  0.0441572 ],\n",
       "       [-0.01620966, -0.0274914 ,  0.04703097, ..., -0.00346784,\n",
       "         0.01711167, -0.04980812]], dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def positional_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m d_model \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n\u001b[1;32m----> 2\u001b[0m d_v, d_q, d_k \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "d_model = 512 # also embedding size\n",
    "d_v, d_q, d_k = 64, 64, 64\n",
    "dff = 2048 # inner feedforward layer dim\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "dropout_rate = 0.1\n",
    "batch_size = 64\n",
    "seq_len = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementaion of dot product attention\n",
    "class DotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, q, k, v, d_k, mask=None):\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 16)\n"
     ]
    }
   ],
   "source": [
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = tf.random.normal((B,T,C))\n",
    "\n",
    "head_size = 16\n",
    "key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "scores = tf.matmul(q, tf.transpose(k, perm=[0, 2, 1])) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = tf.linalg.band_part(tf.ones((T, T)), -1, 0) # lower triangular matrix\n",
    "scores = tf.where(tril == 0, tf.fill(tril.shape, -float('inf')), scores) # mask out the upper triangle\n",
    "scores = tf.nn.softmax(scores, axis=-1) # (B, T, T)\n",
    "\n",
    "v = value(x) # (B, T, 16) \n",
    "out = tf.matmul(scores, v) # (B, T, T) @ (B, T, 16) ---> (B, T, 16)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 211ms/step\n",
      "(32, 10, 64)\n"
     ]
    }
   ],
   "source": [
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(1000, 64, input_length=10))\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0286544 , -0.01695011, -0.01025267, -0.02617158, -0.00404854,\n",
       "        0.04775255, -0.02500275,  0.0416499 ,  0.04432914,  0.0064167 ,\n",
       "        0.02524245,  0.00858472,  0.03022489, -0.04360149, -0.04557949,\n",
       "       -0.04291893,  0.02719354, -0.00238859,  0.01595623,  0.04848215,\n",
       "        0.03093796, -0.03719582,  0.00072558, -0.03027285, -0.04864417,\n",
       "        0.0171029 ,  0.03791413, -0.04314154,  0.01211748,  0.00926459,\n",
       "       -0.03436134, -0.0265825 , -0.00529455,  0.03243819, -0.02915757,\n",
       "        0.00714353, -0.04682566,  0.02050204, -0.04006963,  0.02837286,\n",
       "        0.04162252, -0.02827531,  0.02999714, -0.03455625, -0.02379003,\n",
       "        0.0294604 ,  0.00350038, -0.0319316 , -0.04055564, -0.03999837,\n",
       "        0.00173689, -0.01042766,  0.0348827 , -0.03970977, -0.00249738,\n",
       "        0.03113553, -0.04805944, -0.01816513,  0.00125686,  0.03857238,\n",
       "       -0.00246302, -0.02902842, -0.01994125, -0.04502575], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                          output_dim=d_model, \n",
    "                          input_length=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 8), dtype=float32, numpy=\n",
       "array([[1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [3.2446999e-07, 9.9999964e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [2.1704189e-07, 9.9002355e-01, 9.9762417e-03, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.8885803e-04, 9.7978640e-01, 2.0003926e-02, 2.0735459e-05,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0849527e-01, 9.4697058e-02, 1.0691612e-01, 2.6824350e-02,\n",
       "        6.6306716e-01, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [3.5939179e-02, 7.5873780e-01, 2.0459630e-01, 8.4528483e-06,\n",
       "        7.1826362e-04, 6.8539862e-08, 0.0000000e+00, 0.0000000e+00],\n",
       "       [9.9802607e-01, 9.1235660e-04, 3.6117420e-04, 1.7029686e-05,\n",
       "        1.7039441e-04, 4.1312503e-04, 9.9868579e-05, 0.0000000e+00],\n",
       "       [1.0095829e-03, 2.8786395e-04, 1.5980291e-01, 4.2210758e-02,\n",
       "        5.5063777e-02, 2.6667176e-03, 7.2593540e-01, 1.3023052e-02]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SingleHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, head_size, **kwargs):\n",
    "        super(SingleHeadAttention, self).__init__(**kwargs)\n",
    "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d1a1c06dd7fc6b133a3916b0943de0426aecca2a17bdc4d7dfa8117badf567a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
