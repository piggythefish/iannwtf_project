{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open .npy files from a folder and concatenate all the tokens into one numpy array\n",
    "tokens = np.array(()) # empty numpy array\n",
    "for filename in os.listdir('Dataset_tokenized_BPE'): # iterate through all files in the folder\n",
    "    if filename.endswith('.npy'): # if the file is a .npy file\n",
    "        for i in range(len(np.load(('Dataset_tokenized_BPE/' + filename), allow_pickle=True)[()].get('tokens'))): # iterate through all the tokens in the file\n",
    "            tokens = np.append(tokens, np.load(('Dataset_tokenized_BPE/' + filename), allow_pickle=True)[()].get('tokens')[i]) # append the tokens to the numpy array\n",
    "        continue \n",
    "    else: \n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[224 212  50 227 216  52 231 218]\n",
      " [ 50 106 251  43 244  46 261 202]\n",
      " [198  45 224 200  48 331  43 220]\n",
      " [122 204  59 264 205  47  94 122]], shape=(4, 8), dtype=int32) tf.Tensor(\n",
      "[[212  50 227 216  52 231 218  45]\n",
      " [106 251  43 244  46 261 202  51]\n",
      " [ 45 224 200  48 331  43 220 204]\n",
      " [204  59 264 205  47  94 122 206]], shape=(4, 8), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.layers.attention.multi_head_attention.MultiHeadAttention at 0x2388bf03640>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = tokens.astype(int) # convert the numpy array to integers\n",
    "\n",
    "data = tf.cast(tokens, tf.int32) # put tokens into a tensor and cast to int32\n",
    "# print(data)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n] # first 90% of data\n",
    "# print(train_data)\n",
    "val_data = data[n:] # last 10% of data\n",
    "# print(val_data)\n",
    "\n",
    "BLOCK_SIZE = 8\n",
    "BATCH_SIZE = 4\n",
    "    \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = tf.random.uniform((BATCH_SIZE,), maxval=len(data)-BLOCK_SIZE, dtype=tf.int32)\n",
    "    x = tf.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = tf.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "inputs, targets = get_batch('train')\n",
    "print(inputs, targets)\n",
    "\n",
    "tf.keras.layers.MultiHeadAttention(1, 8) # 1 head, 8 units per head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 8), dtype=int32, numpy=\n",
       "array([[212,  50, 227, 216,  52, 231, 218,  45],\n",
       "       [106, 251,  43, 244,  46, 261, 202,  51],\n",
       "       [ 45, 224, 200,  48, 331,  43, 220, 204],\n",
       "       [204,  59, 264, 205,  47,  94, 122, 206]])>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [50] the target is: 229\n",
      "when input is [ 50 229] the target is: 57\n",
      "when input is [ 50 229  57] the target is: 289\n",
      "when input is [ 50 229  57 289] the target is: 214\n",
      "when input is [ 50 229  57 289 214] the target is: 50\n",
      "when input is [ 50 229  57 289 214  50] the target is: 228\n",
      "when input is [ 50 229  57 289 214  50 228] the target is: 57\n",
      "when input is [ 50 229  57 289 214  50 228  57] the target is: 253\n",
      "when input is [225] the target is: 65\n",
      "when input is [225  65] the target is: 261\n",
      "when input is [225  65 261] the target is: 216\n",
      "when input is [225  65 261 216] the target is: 57\n",
      "when input is [225  65 261 216  57] the target is: 321\n",
      "when input is [225  65 261 216  57 321] the target is: 60\n",
      "when input is [225  65 261 216  57 321  60] the target is: 225\n",
      "when input is [225  65 261 216  57 321  60 225] the target is: 65\n",
      "when input is [24] the target is: 337\n",
      "when input is [ 24 337] the target is: 12\n",
      "when input is [ 24 337  12] the target is: 224\n",
      "when input is [ 24 337  12 224] the target is: 196\n",
      "when input is [ 24 337  12 224 196] the target is: 24\n",
      "when input is [ 24 337  12 224 196  24] the target is: 377\n",
      "when input is [ 24 337  12 224 196  24 377] the target is: 12\n",
      "when input is [ 24 337  12 224 196  24 377  12] the target is: 224\n",
      "when input is [313] the target is: 16\n",
      "when input is [313  16] the target is: 221\n",
      "when input is [313  16 221] the target is: 188\n",
      "when input is [313  16 221 188] the target is: 28\n",
      "when input is [313  16 221 188  28] the target is: 400\n",
      "when input is [313  16 221 188  28 400] the target is: 31\n",
      "when input is [313  16 221 188  28 400  31] the target is: 221\n",
      "when input is [313  16 221 188  28 400  31 221] the target is: 192\n"
     ]
    }
   ],
   "source": [
    "for b in range(BATCH_SIZE): # batch dimension\n",
    "    for t in range(BLOCK_SIZE): # time dimension\n",
    "        context = inputs[b, :t+1]\n",
    "        target = targets[b,t]\n",
    "        print(f\"when input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model, activation='relu'),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch('train')[1].shape\n",
    "inputs = tf.keras.layers.Input(shape=get_batch('train')[0].shape)\n",
    "outputs= FeedForward(8, 32)(inputs)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# model.fit(get_batch('train'),\n",
    "#                 epochs=20,\n",
    "#                 validation_data=get_batch('val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 8, 8), dtype=float32, numpy=\n",
       "array([[[-0.01894864, -0.00758777, -0.02547537, -0.03709454,\n",
       "          0.02172568, -0.02753102, -0.03409537, -0.02683078],\n",
       "        [-0.00862032, -0.0002634 ,  0.01840265, -0.03801345,\n",
       "         -0.02777342, -0.01620481, -0.03822521, -0.00355053],\n",
       "        [-0.04214961, -0.02287751,  0.0376593 ,  0.04500557,\n",
       "         -0.0372509 ,  0.01384381,  0.02985987,  0.025936  ],\n",
       "        [-0.02272687,  0.01554424,  0.04035195,  0.00617802,\n",
       "         -0.04731195,  0.00525496, -0.03953175, -0.01883389],\n",
       "        [ 0.01639735, -0.04936351, -0.04055194,  0.00983968,\n",
       "          0.04348755,  0.03789325,  0.04307434,  0.03306111],\n",
       "        [-0.02681906,  0.03159245,  0.01588969,  0.01478067,\n",
       "         -0.02102259,  0.01165945, -0.00497264,  0.02824814],\n",
       "        [-0.04940389, -0.01953031, -0.01591098, -0.04055402,\n",
       "          0.01773492, -0.04411858,  0.00464853,  0.03141815],\n",
       "        [-0.01987718, -0.01801397, -0.03207477,  0.04858736,\n",
       "         -0.02665482, -0.03549849,  0.00607162,  0.04413522]],\n",
       "\n",
       "       [[-0.04214961, -0.02287751,  0.0376593 ,  0.04500557,\n",
       "         -0.0372509 ,  0.01384381,  0.02985987,  0.025936  ],\n",
       "        [ 0.00696979,  0.04609538, -0.03593088,  0.01123248,\n",
       "         -0.00913458,  0.02940736,  0.04278651, -0.02426615],\n",
       "        [-0.01055218,  0.04942266,  0.04279624,  0.03658236,\n",
       "          0.0334524 ,  0.03672451,  0.00969028,  0.04851569],\n",
       "        [ 0.02825313,  0.03627584, -0.04231173,  0.03225071,\n",
       "         -0.00388987, -0.00900995,  0.00940634,  0.03488245],\n",
       "        [-0.00739852,  0.00426742,  0.03209325, -0.03041312,\n",
       "          0.03428825,  0.02782771, -0.01298195,  0.03537873],\n",
       "        [ 0.02078177, -0.02244501,  0.01285807,  0.02821236,\n",
       "         -0.03828294, -0.03729291, -0.03855038, -0.0194232 ],\n",
       "        [ 0.01459125,  0.03511183,  0.01913119,  0.02162624,\n",
       "         -0.019484  ,  0.03236404,  0.01661453,  0.00999262],\n",
       "        [ 0.00896194, -0.04517153, -0.04551278,  0.02224138,\n",
       "          0.01872106, -0.01917309, -0.0226028 , -0.04563475]],\n",
       "\n",
       "       [[ 0.0039227 ,  0.03266177,  0.04198435,  0.0070109 ,\n",
       "          0.02853603,  0.03128034, -0.0128198 ,  0.03344071],\n",
       "        [-0.03146972, -0.03113427, -0.01367901,  0.03582639,\n",
       "          0.00762846,  0.0276294 , -0.00710989, -0.04063006],\n",
       "        [-0.01894864, -0.00758777, -0.02547537, -0.03709454,\n",
       "          0.02172568, -0.02753102, -0.03409537, -0.02683078],\n",
       "        [-0.03169318,  0.04214155, -0.00859744, -0.04990248,\n",
       "         -0.02467876,  0.0264057 ,  0.03576216, -0.00841997],\n",
       "        [ 0.03121468,  0.01546854, -0.00024116, -0.00730164,\n",
       "          0.01544681,  0.04857585,  0.00601159,  0.03750562],\n",
       "        [ 0.04127553, -0.00511521, -0.04633529,  0.04683961,\n",
       "         -0.02169701,  0.02782805, -0.0389635 , -0.04721532],\n",
       "        [ 0.02825313,  0.03627584, -0.04231173,  0.03225071,\n",
       "         -0.00388987, -0.00900995,  0.00940634,  0.03488245],\n",
       "        [ 0.04928963, -0.01754475, -0.03815619, -0.03335201,\n",
       "          0.0091228 , -0.03438582, -0.01191882, -0.01508454]],\n",
       "\n",
       "       [[ 0.04229638,  0.03890368, -0.01288436, -0.02015338,\n",
       "          0.03349129,  0.02610774,  0.00755668,  0.0238005 ],\n",
       "        [ 0.0101578 ,  0.04623692, -0.03499972,  0.03985577,\n",
       "         -0.01665988,  0.0355058 , -0.04402503,  0.03455896],\n",
       "        [-0.00480124, -0.03672792,  0.04048829,  0.02486323,\n",
       "         -0.02048128,  0.00371182,  0.01134759,  0.0386656 ],\n",
       "        [ 0.02264521, -0.02224609, -0.03682969, -0.02541372,\n",
       "          0.02124653, -0.03088404, -0.04112088, -0.00818003],\n",
       "        [-0.02411979,  0.00602962, -0.03175773, -0.02151809,\n",
       "         -0.03122643, -0.0250923 , -0.04527197,  0.01950089],\n",
       "        [-0.04620492, -0.02598965,  0.03165333,  0.02066484,\n",
       "         -0.03423208,  0.0212704 ,  0.00760299,  0.01049454],\n",
       "        [ 0.03789312, -0.00437363,  0.01537481, -0.00062361,\n",
       "          0.02495983, -0.0490878 ,  0.02820847, -0.0232185 ],\n",
       "        [ 0.04229638,  0.03890368, -0.01288436, -0.02015338,\n",
       "          0.03349129,  0.02610774,  0.00755668,  0.0238005 ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.keras.layers.Embedding(1000,8)\n",
    "em(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 8), dtype=float32, numpy=\n",
       "array([[ 0.4418457 ,  0.7790316 , -2.3305297 ,  0.9312438 ,  0.85393375,\n",
       "        -0.28479034, -0.11583445, -0.27490097],\n",
       "       [-0.9555565 , -0.74374133,  1.0320394 , -1.6921114 ,  1.2062762 ,\n",
       "        -0.04276661,  1.0634617 ,  0.13239864],\n",
       "       [-0.06966723, -0.73711395,  0.16599175,  0.73687583, -0.7739256 ,\n",
       "         2.022661  , -1.4745574 ,  0.12973653],\n",
       "       [-0.10219621,  1.1680634 , -0.9239531 ,  1.6237562 ,  0.766556  ,\n",
       "        -0.38831973, -1.2936971 , -0.8502097 ]], dtype=float32)>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = FeedForward(d_model= 8, dff =32)\n",
    "x = ffn(tf.cast(inputs, tf.float32))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahead = BaseAttention(num_heads=1, key_dim=8)\n",
    "ahead = CausalSelfAttention(num_heads=1, key_dim=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context='gay'):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    # x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"query\" \"                 f\"(type EinsumDense).\n\ncannot compute Einsum as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:Einsum]\n\nCall arguments received by layer \"query\" \"                 f\"(type EinsumDense):\n  • inputs=tf.Tensor(shape=(4, 8), dtype=int32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dl \u001b[39m=\u001b[39m DecoderLayer(d_model\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, num_heads\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, dff\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m dl(inputs)\n",
      "File \u001b[1;32mc:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn [45], line 23\u001b[0m, in \u001b[0;36mDecoderLayer.call\u001b[1;34m(self, x, context)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x, context\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgay\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcausal_self_attention(x\u001b[39m=\u001b[39;49mx)\n\u001b[0;32m     24\u001b[0m   \u001b[39m# x = self.cross_attention(x=x, context=context)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m   \u001b[39m# Cache the last attention scores for plotting later\u001b[39;00m\n\u001b[0;32m     27\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_attn_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_attention\u001b[39m.\u001b[39mlast_attn_scores\n",
      "Cell \u001b[1;32mIn [33], line 3\u001b[0m, in \u001b[0;36mCausalSelfAttention.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m----> 3\u001b[0m   attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmha(\n\u001b[0;32m      4\u001b[0m       query\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m      5\u001b[0m       value\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m      6\u001b[0m       key\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m      7\u001b[0m       use_causal_mask \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      8\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd([x, attn_output])\n\u001b[0;32m      9\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm(x)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"query\" \"                 f\"(type EinsumDense).\n\ncannot compute Einsum as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:Einsum]\n\nCall arguments received by layer \"query\" \"                 f\"(type EinsumDense):\n  • inputs=tf.Tensor(shape=(4, 8), dtype=int32)"
     ]
    }
   ],
   "source": [
    "dl = DecoderLayer(d_model=4, num_heads=8, dff=8)\n",
    "dl(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d1a1c06dd7fc6b133a3916b0943de0426aecca2a17bdc4d7dfa8117badf567a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
