{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import miditok as midi\n",
    "from functions import get_tokens, split_input_target, positional_encoding, masked_loss, masked_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the tokens and some information about our data and some basic prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smallest token:1.0, biggest token:499.0, number of unique tokens:(495,), missing tokens:{0, 178, 180, 182, 186}\n",
      "number of tokens: 1820490\n",
      "number of pieces: 295\n",
      "average tokens per piece: 6171.152542372882\n",
      "{1.0: 3257, 2.0: 3, 3.0: 17, 4.0: 4, 5.0: 65, 6.0: 117, 7.0: 150, 8.0: 273, 9.0: 310, 10.0: 1049, 11.0: 729, 12.0: 1551, 13.0: 1740, 14.0: 2061, 15.0: 2405, 16.0: 2107, 17.0: 3680, 18.0: 3281, 19.0: 3793, 20.0: 4097, 21.0: 4237, 22.0: 5641, 23.0: 3988, 24.0: 7081, 25.0: 7304, 26.0: 7682, 27.0: 8112, 28.0: 6580, 29.0: 11021, 30.0: 9102, 31.0: 12150, 32.0: 12375, 33.0: 12382, 34.0: 15197, 35.0: 11284, 36.0: 19087, 37.0: 15810, 38.0: 18261, 39.0: 18602, 40.0: 16561, 41.0: 20010, 42.0: 16571, 43.0: 22740, 44.0: 18633, 45.0: 20574, 46.0: 21075, 47.0: 16291, 48.0: 22034, 49.0: 17170, 50.0: 20122, 51.0: 17637, 52.0: 17425, 53.0: 21719, 54.0: 15413, 55.0: 20931, 56.0: 14869, 57.0: 16683, 58.0: 16490, 59.0: 11508, 60.0: 15490, 61.0: 10034, 62.0: 12191, 63.0: 9226, 64.0: 7897, 65.0: 8812, 66.0: 5689, 67.0: 6786, 68.0: 4615, 69.0: 4479, 70.0: 4003, 71.0: 2746, 72.0: 2829, 73.0: 2352, 74.0: 1934, 75.0: 1670, 76.0: 1168, 77.0: 1029, 78.0: 1140, 79.0: 702, 80.0: 799, 81.0: 671, 82.0: 341, 83.0: 101, 84.0: 82, 85.0: 16, 86.0: 8, 87.0: 1, 88.0: 1, 89.0: 295, 90.0: 8, 91.0: 37, 92.0: 289, 93.0: 1325, 94.0: 1969, 95.0: 2840, 96.0: 5957, 97.0: 7062, 98.0: 8003, 99.0: 4836, 100.0: 6724, 101.0: 7029, 102.0: 6834, 103.0: 7686, 104.0: 6868, 105.0: 5829, 106.0: 5291, 107.0: 5705, 108.0: 4781, 109.0: 4401, 110.0: 3843, 111.0: 2712, 112.0: 3284, 113.0: 2110, 114.0: 1357, 115.0: 768, 116.0: 347, 117.0: 152, 118.0: 48, 119.0: 11, 120.0: 2, 121.0: 1, 122.0: 1914, 123.0: 982, 124.0: 2319, 125.0: 1976, 126.0: 2519, 127.0: 4013, 128.0: 2136, 129.0: 2468, 130.0: 536, 131.0: 1023, 132.0: 460, 133.0: 5859, 134.0: 157, 135.0: 1447, 136.0: 731, 137.0: 5707, 138.0: 132, 139.0: 283, 140.0: 86, 141.0: 1360, 142.0: 24, 143.0: 178, 144.0: 84, 145.0: 5873, 146.0: 36, 147.0: 41, 148.0: 13, 149.0: 294, 150.0: 19, 151.0: 117, 152.0: 23, 153.0: 2451, 154.0: 59, 155.0: 318, 156.0: 15, 157.0: 326, 158.0: 7, 159.0: 40, 160.0: 20, 161.0: 630, 162.0: 9, 163.0: 35, 164.0: 3, 165.0: 70, 166.0: 11, 167.0: 24, 168.0: 2, 169.0: 186, 170.0: 7, 171.0: 14, 172.0: 1, 173.0: 36, 174.0: 1, 175.0: 1, 176.0: 295, 177.0: 18, 179.0: 7, 181.0: 1, 183.0: 8, 184.0: 2, 185.0: 60, 187.0: 2554, 188.0: 10259, 189.0: 3416, 190.0: 15899, 191.0: 5561, 192.0: 11474, 193.0: 3592, 194.0: 16074, 195.0: 2608, 196.0: 11691, 197.0: 3314, 198.0: 18368, 199.0: 5720, 200.0: 10274, 201.0: 3762, 202.0: 14235, 203.0: 2520, 204.0: 11539, 205.0: 3248, 206.0: 17693, 207.0: 5603, 208.0: 10708, 209.0: 3541, 210.0: 14934, 211.0: 2669, 212.0: 12580, 213.0: 3300, 214.0: 16630, 215.0: 5826, 216.0: 9990, 217.0: 3852, 218.0: 16958, 219.0: 8496, 220.0: 9763, 221.0: 10178, 222.0: 10989, 223.0: 9816, 224.0: 11390, 225.0: 11408, 226.0: 11251, 227.0: 14065, 228.0: 11552, 229.0: 12404, 230.0: 4869, 231.0: 11899, 232.0: 4835, 233.0: 4914, 234.0: 4257, 235.0: 5832, 236.0: 11399, 237.0: 10860, 238.0: 5667, 239.0: 10760, 240.0: 10665, 241.0: 7464, 242.0: 7115, 243.0: 9607, 244.0: 9075, 245.0: 9028, 246.0: 8932, 247.0: 8856, 248.0: 5998, 249.0: 6621, 250.0: 8011, 251.0: 6967, 252.0: 6587, 253.0: 7167, 254.0: 7095, 255.0: 6327, 256.0: 6279, 257.0: 5999, 258.0: 5811, 259.0: 5700, 260.0: 5530, 261.0: 5448, 262.0: 4534, 263.0: 4460, 264.0: 4369, 265.0: 4061, 266.0: 3985, 267.0: 3873, 268.0: 3723, 269.0: 3680, 270.0: 3608, 271.0: 3571, 272.0: 3470, 273.0: 3457, 274.0: 3329, 275.0: 3321, 276.0: 3319, 277.0: 3280, 278.0: 3182, 279.0: 3164, 280.0: 3121, 281.0: 2892, 282.0: 389, 283.0: 2797, 284.0: 2753, 285.0: 2687, 286.0: 2683, 287.0: 2676, 288.0: 2662, 289.0: 2555, 290.0: 2529, 291.0: 2417, 292.0: 2387, 293.0: 2333, 294.0: 2210, 295.0: 2122, 296.0: 2029, 297.0: 1807, 298.0: 1795, 299.0: 1789, 300.0: 1747, 301.0: 1575, 302.0: 1555, 303.0: 1517, 304.0: 1514, 305.0: 1493, 306.0: 1470, 307.0: 1458, 308.0: 1447, 309.0: 1432, 310.0: 1428, 311.0: 1410, 312.0: 1403, 313.0: 1400, 314.0: 1386, 315.0: 1373, 316.0: 1372, 317.0: 1362, 318.0: 1354, 319.0: 1347, 320.0: 1346, 321.0: 1337, 322.0: 1337, 323.0: 1326, 324.0: 1316, 325.0: 1296, 326.0: 1288, 327.0: 1278, 328.0: 1278, 329.0: 1268, 330.0: 1255, 331.0: 1242, 332.0: 1234, 333.0: 1231, 334.0: 1231, 335.0: 1225, 336.0: 1217, 337.0: 1206, 338.0: 1199, 339.0: 1192, 340.0: 1186, 341.0: 1175, 342.0: 1175, 343.0: 1174, 344.0: 1172, 345.0: 1168, 346.0: 1164, 347.0: 1162, 348.0: 1157, 349.0: 1157, 350.0: 1155, 351.0: 1154, 352.0: 1147, 353.0: 1146, 354.0: 1140, 355.0: 1139, 356.0: 1136, 357.0: 1134, 358.0: 1132, 359.0: 1130, 360.0: 1126, 361.0: 1126, 362.0: 1125, 363.0: 1123, 364.0: 598, 365.0: 1120, 366.0: 1112, 367.0: 1107, 368.0: 1103, 369.0: 1102, 370.0: 1102, 371.0: 1096, 372.0: 1095, 373.0: 1089, 374.0: 1084, 375.0: 1078, 376.0: 1075, 377.0: 1074, 378.0: 1073, 379.0: 1071, 380.0: 1067, 381.0: 1064, 382.0: 1062, 383.0: 1061, 384.0: 1053, 385.0: 1051, 386.0: 1051, 387.0: 1038, 388.0: 1037, 389.0: 1035, 390.0: 1034, 391.0: 1032, 392.0: 1031, 393.0: 1028, 394.0: 1027, 395.0: 1025, 396.0: 1020, 397.0: 1019, 398.0: 1015, 399.0: 1015, 400.0: 1011, 401.0: 1006, 402.0: 1000, 403.0: 994, 404.0: 987, 405.0: 983, 406.0: 977, 407.0: 974, 408.0: 973, 409.0: 965, 410.0: 961, 411.0: 958, 412.0: 956, 413.0: 953, 414.0: 946, 415.0: 942, 416.0: 940, 417.0: 937, 418.0: 931, 419.0: 930, 420.0: 930, 421.0: 924, 422.0: 921, 423.0: 920, 424.0: 915, 425.0: 914, 426.0: 914, 427.0: 910, 428.0: 903, 429.0: 901, 430.0: 895, 431.0: 894, 432.0: 894, 433.0: 893, 434.0: 891, 435.0: 890, 436.0: 887, 437.0: 884, 438.0: 883, 439.0: 882, 440.0: 880, 441.0: 879, 442.0: 878, 443.0: 876, 444.0: 875, 445.0: 875, 446.0: 874, 447.0: 864, 448.0: 863, 449.0: 862, 450.0: 850, 451.0: 846, 452.0: 846, 453.0: 845, 454.0: 842, 455.0: 841, 456.0: 840, 457.0: 840, 458.0: 839, 459.0: 838, 460.0: 835, 461.0: 835, 462.0: 834, 463.0: 833, 464.0: 833, 465.0: 832, 466.0: 825, 467.0: 825, 468.0: 824, 469.0: 822, 470.0: 821, 471.0: 821, 472.0: 817, 473.0: 816, 474.0: 814, 475.0: 813, 476.0: 807, 477.0: 806, 478.0: 805, 479.0: 804, 480.0: 803, 481.0: 802, 482.0: 802, 483.0: 800, 484.0: 798, 485.0: 795, 486.0: 795, 487.0: 795, 488.0: 794, 489.0: 794, 490.0: 793, 491.0: 792, 492.0: 789, 493.0: 788, 494.0: 787, 495.0: 783, 496.0: 783, 497.0: 782, 498.0: 780, 499.0: 778}\n"
     ]
    }
   ],
   "source": [
    "tokens, piece_counter = get_tokens() # get all tokens from the dataset\n",
    "unique_tokens, count = np.unique(tokens, return_counts=True)\n",
    "vocab_size = int(np.max(tokens)) + 1 # get the size of the vocabulary\n",
    "\n",
    "print(\"smallest token:{}, biggest token:{}, number of unique tokens:{}, missing tokens:{}\".format(np.min(tokens),np.max(tokens),np.unique(tokens).shape,set(range(0, 500))-set(tokens))) # print some information about the tokens\n",
    "print(\"number of tokens: {}\".format(len(tokens))) # print the number of tokens\n",
    "print(\"number of pieces: {}\".format(piece_counter)) # print the number of pieces\n",
    "print(\"average tokens per piece: {}\".format(len(tokens)/piece_counter)) # print average tokens per piece\n",
    "print(dict(zip(unique_tokens, count))) # print the number of occurences of each token\n",
    "\n",
    "data = tf.cast(tokens, tf.int32) # put tokens into a tensor and cast to int32\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n] # first 90% of data\n",
    "val_data = data[n:] # last 10% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# postional encoding function\n",
    "def positional_encoding(length, d_model):\n",
    "    pos = np.arange(length)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "pos_encoding = positional_encoding(seq_length, d_model)\n",
    "\n",
    "# plot the positional encoding\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, d_model))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer via subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer for embedding and positional encoding\n",
    "class PostionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, block_size):\n",
    "        super(PostionalEmbedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.block_size = block_size\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                          output_dim=d_model, # each token gets a 512-vector embedding\n",
    "                          input_length=block_size)\n",
    "        self.pos_encoding = positional_encoding(block_size, d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.embedding(x) + self.pos_encoding[:, :self.block_size, :]\n",
    "    \n",
    "# dot-product attention\n",
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.head_size = d_model // n_heads\n",
    "        self.key = tf.keras.layers.Dense(units=self.head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=self.head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=self.head_size, use_bias=False)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute scores\n",
    "        scores = tf.matmul(q,tf.transpose(k, perm=[0, 2, 1])) / tf.math.sqrt(tf.cast(self.head_size, tf.float32))\n",
    "        tril = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
    "        scores = tf.where(tril == 0, tf.fill(tril.shape, -float('inf')), scores)\n",
    "        scores = tf.nn.softmax(scores, axis=-1)\n",
    "        # weighted sum of values\n",
    "        v = self.value(x)\n",
    "        return tf.matmul(scores, v)\n",
    "\n",
    "# multi-head attention\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = [ScaledDotProductAttention(d_model, n_heads) for _ in range(n_heads)]\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = tf.concat([head(x) for head in self.heads], axis=-1)\n",
    "        return self.dense(x)\n",
    "    \n",
    "# feed-forward network\n",
    "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=dff, activation=tf.nn.gelu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# decoder layer\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, dff, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = FeedForwardNetwork(d_model, dff, dropout_rate)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        res_x = x\n",
    "        x = self.layernorm1(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.add([res_x, x])\n",
    "        res_x = x\n",
    "        x = self.layernorm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.add([res_x, x])\n",
    "        return x\n",
    "    \n",
    "# decoder\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, dff, dropout_rate, n_layers, block_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = PostionalEmbedding(vocab_size, d_model, block_size)\n",
    "        self.layers = [DecoderLayer(d_model, n_heads, dff, dropout_rate) for _ in range(n_layers)]\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "# transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, dff, dropout_rate, n_layers, block_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.decoder = Decoder(vocab_size, d_model, n_heads, dff, dropout_rate, n_layers, block_size)\n",
    "        self.final_dense = tf.keras.layers.Dense(units=vocab_size)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.decoder(x)\n",
    "        logits = self.final_dense(x)\n",
    "            \n",
    "        try:      \n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits\n",
    "    \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 128 # also embedding size\n",
    "dff = 512 # inner feedforward layer dim\n",
    "n_heads = 8 # number of heads in the multihead attention layer\n",
    "d_v, d_q, d_k = d_model // n_heads , d_model // n_heads , d_model // n_heads # dimension of the query, key and value vectors \n",
    "n_layers = 6 # number of layers\n",
    "dropout_rate = 0.1 # dropout rate\n",
    "epochs = 100\n",
    "seq_length = 512 # length of the sequence\n",
    "batch_size = 64 # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "    return ds.shuffle(4096).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# create tf.data.Dataset objects for training and validation\n",
    "train_ds, val_ds = tf.data.Dataset.from_tensor_slices(train_data), tf.data.Dataset.from_tensor_slices(val_data)\n",
    "# generate sequences of length with .batch and then split the sequences into input and target\n",
    "train_ds, val_ds = train_ds.batch(seq_length+1, drop_remainder=True).map(split_input_target), val_ds.batch(seq_length+1, drop_remainder=True).map(split_input_target) \n",
    "# shuffle, batch and prefetch as usual\n",
    "train_ds, val_ds = make_batches(train_ds), make_batches(val_ds)\n",
    "\n",
    "# set the learning rate to our creeted custom schedule\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "# create the optimizer with the learning rate and the other parameters as specified in the paper\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "# create the model with the specified parameters \n",
    "model = Transformer(vocab_size, d_model, n_heads, dff, dropout_rate, n_layers, seq_length)\n",
    "\n",
    "model.build(input_shape=(None, seq_length))\n",
    "\n",
    "model.load_weights('dmodel 128 dff 512 nheads 8 nlayers 6 dropout 0.1 epochs 100 seqlen 512 batch 64.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf.data.Dataset objects for training and validation\n",
    "train_ds, val_ds = tf.data.Dataset.from_tensor_slices(train_data), tf.data.Dataset.from_tensor_slices(val_data)\n",
    "# generate sequences of length with .batch and then split the sequences into input and target\n",
    "train_ds, val_ds = train_ds.batch(seq_length+1, drop_remainder=True).map(split_input_target), val_ds.batch(seq_length+1, drop_remainder=True).map(split_input_target) \n",
    "# shuffle, batch and prefetch as usual\n",
    "train_ds, val_ds = make_batches(train_ds), make_batches(val_ds)\n",
    "\n",
    "# set the learning rate to our creeted custom schedule\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "# create the optimizer with the learning rate and the other parameters as specified in the paper\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "# create the model with the specified parameters \n",
    "model = Transformer(vocab_size, d_model, n_heads, dff, dropout_rate, n_layers, seq_length)\n",
    "\n",
    "model.build(input_shape=(None, seq_length))\n",
    "\n",
    "model.load_weights('dmodel 128 dff 512 nheads 8 nlayers 6 dropout 0.1 epochs 100 seqlen 512 batch 64.h5')\n",
    "\n",
    "# save weights of the model after every epoch\n",
    "checkpoint_path = \"checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(transformer=model,\n",
    "                            optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=None)\n",
    "\n",
    "# compile the model with the masked loss and masked accuracy\n",
    "model.compile(optimizer=optimizer, loss=masked_loss, metrics=[masked_accuracy])\n",
    "\n",
    "# train the model \n",
    "model.fit(train_ds, epochs=epochs, validation_data=val_ds)\n",
    "\n",
    "# save the model\n",
    "model.save_weights('dmodel:{} dff:{} nheads:{} nlayers:{} dropout:{} epochs:{} seqlen:{} batch:{}.h5'.format(d_model, dff, n_heads, n_layers, dropout_rate, epochs, seq_length, batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 512\n",
    "start_token = tf.constant([[89] + [3] + [0] * (seq_length)], dtype=tf.int64)\n",
    "np.delete(start_token, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 511\n",
    "start_token = tf.constant([[89] + [0] * (seq_length - 1)], dtype=tf.int64)\n",
    "\n",
    "def generate_tokens_greedy(model, start_token, max_len):\n",
    "    for i in range(max_len):\n",
    "        logits = model(start_token) # generate logits for the next token\n",
    "        probs = tf.nn.softmax(logits, axis = -1) # get the probabilities \n",
    "        argmax = tf.argmax(probs, axis = -1) # get the token with the highest probability\n",
    "        start_token_numpy = start_token.numpy() # convert the tensor to numpy array\n",
    "        start_token_numpy[:, i+1] = argmax[:,i].numpy() # add the predicted token to the sequence\n",
    "        start_token = tf.constant(start_token_numpy, dtype=tf.int64) # convert the numpy array to tensor\n",
    "        if argmax[:,i].numpy() == 176: # stop when the end token is predicted\n",
    "            break\n",
    "    return start_token\n",
    "\n",
    "def generate_tokens_sampling(model, start_token, max_len):\n",
    "    for i in range(max_len):\n",
    "        logits = model(start_token) # generate logits for the next token\n",
    "        start_token_numpy = start_token.numpy() # convert the tensor to numpy array\n",
    "        start_token_numpy[:, i+1] = tf.random.categorical(logits[:,i], num_samples=1).numpy() # sample a token from the logits and add it to the sequence\n",
    "        start_token = tf.constant(start_token_numpy, dtype=tf.int64) # convert the numpy array to tensor\n",
    "        if start_token_numpy[:, i+1] == 176: # stop when the end token is predicted\n",
    "            break\n",
    "    return start_token\n",
    "\n",
    "sample_song = generate_tokens_sampling(model, start_token, max_len)\n",
    "sample_song.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import REMI, MIDITokenizer\n",
    "tokenizer = REMI()\n",
    "tokenizer.load_params('Dataset_tokenized_BPE/config.txt')\n",
    "coverted = tokenizer(sample_song, [(0, False)])\n",
    "coverted.dump('sample_song_sampling.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import REMI, MIDITokenizer\n",
    "from miditok.utils import get_midi_programs\n",
    "from miditoolkit import MidiFile\n",
    "from pathlib import Path\n",
    "tokenizer = REMI()\n",
    "cock = MidiFile('Final_Project/Dataset/chopin/chp_op18.mid')\n",
    "schwanz = tokenizer(MidiFile('Final_Project/Dataset/chopin/chp_op31.mid'))\n",
    "tokens = tokenizer(cock) \n",
    "coverted_back = tokenizer(tokens, [(0, False), (0, False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def json_rdy(sample):\n",
    "    sample = sample.numpy().tolist()[0]\n",
    "    sample = {\"tokens\" : [sample,sample], \"programs\" : [[0, False],[0, False]]}\n",
    "    with open('{}.json', 'w'.format(sample)) as fp:\n",
    "        json.dump(json_rdy(sample), fp)    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [[89,\n",
       "   218,\n",
       "   16,\n",
       "   96,\n",
       "   153,\n",
       "   28,\n",
       "   97,\n",
       "   153,\n",
       "   194,\n",
       "   28,\n",
       "   96,\n",
       "   252,\n",
       "   32,\n",
       "   97,\n",
       "   248,\n",
       "   21,\n",
       "   94,\n",
       "   157,\n",
       "   28,\n",
       "   96,\n",
       "   157,\n",
       "   33,\n",
       "   97,\n",
       "   153,\n",
       "   1,\n",
       "   192,\n",
       "   28,\n",
       "   96,\n",
       "   139,\n",
       "   193,\n",
       "   28,\n",
       "   96,\n",
       "   138,\n",
       "   35,\n",
       "   96,\n",
       "   138,\n",
       "   36,\n",
       "   97,\n",
       "   138,\n",
       "   194,\n",
       "   16,\n",
       "   258,\n",
       "   28,\n",
       "   231,\n",
       "   198,\n",
       "   28,\n",
       "   456,\n",
       "   40,\n",
       "   260,\n",
       "   218,\n",
       "   16,\n",
       "   258,\n",
       "   28,\n",
       "   231,\n",
       "   190,\n",
       "   28,\n",
       "   260,\n",
       "   36,\n",
       "   456,\n",
       "   16,\n",
       "   258,\n",
       "   28,\n",
       "   231,\n",
       "   206,\n",
       "   28,\n",
       "   260,\n",
       "   36,\n",
       "   492,\n",
       "   20,\n",
       "   297,\n",
       "   32,\n",
       "   231,\n",
       "   214,\n",
       "   28,\n",
       "   260,\n",
       "   36,\n",
       "   429,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   227,\n",
       "   190,\n",
       "   32,\n",
       "   260,\n",
       "   39,\n",
       "   456,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   231,\n",
       "   206,\n",
       "   32,\n",
       "   293,\n",
       "   38,\n",
       "   492,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   231,\n",
       "   214,\n",
       "   28,\n",
       "   260,\n",
       "   34,\n",
       "   429,\n",
       "   20,\n",
       "   94,\n",
       "   169,\n",
       "   28,\n",
       "   95,\n",
       "   169,\n",
       "   33,\n",
       "   95,\n",
       "   169,\n",
       "   208,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   38,\n",
       "   291,\n",
       "   44,\n",
       "   264,\n",
       "   210,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   231,\n",
       "   214,\n",
       "   32,\n",
       "   293,\n",
       "   38,\n",
       "   260,\n",
       "   44,\n",
       "   429,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   39,\n",
       "   96,\n",
       "   127,\n",
       "   44,\n",
       "   96,\n",
       "   127,\n",
       "   192,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   39,\n",
       "   93,\n",
       "   127,\n",
       "   196,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   39,\n",
       "   93,\n",
       "   127,\n",
       "   44,\n",
       "   291,\n",
       "   198,\n",
       "   32,\n",
       "   92,\n",
       "   127,\n",
       "   39,\n",
       "   93,\n",
       "   127,\n",
       "   43,\n",
       "   117,\n",
       "   127,\n",
       "   204,\n",
       "   20,\n",
       "   95,\n",
       "   137,\n",
       "   32,\n",
       "   93,\n",
       "   137,\n",
       "   176,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [89,\n",
       "   218,\n",
       "   16,\n",
       "   96,\n",
       "   153,\n",
       "   28,\n",
       "   97,\n",
       "   153,\n",
       "   194,\n",
       "   28,\n",
       "   96,\n",
       "   252,\n",
       "   32,\n",
       "   97,\n",
       "   248,\n",
       "   21,\n",
       "   94,\n",
       "   157,\n",
       "   28,\n",
       "   96,\n",
       "   157,\n",
       "   33,\n",
       "   97,\n",
       "   153,\n",
       "   1,\n",
       "   192,\n",
       "   28,\n",
       "   96,\n",
       "   139,\n",
       "   193,\n",
       "   28,\n",
       "   96,\n",
       "   138,\n",
       "   35,\n",
       "   96,\n",
       "   138,\n",
       "   36,\n",
       "   97,\n",
       "   138,\n",
       "   194,\n",
       "   16,\n",
       "   258,\n",
       "   28,\n",
       "   231,\n",
       "   198,\n",
       "   28,\n",
       "   456,\n",
       "   40,\n",
       "   260,\n",
       "   218,\n",
       "   16,\n",
       "   258,\n",
       "   28,\n",
       "   231,\n",
       "   190,\n",
       "   28,\n",
       "   260,\n",
       "   36,\n",
       "   456,\n",
       "   16,\n",
       "   258,\n",
       "   28,\n",
       "   231,\n",
       "   206,\n",
       "   28,\n",
       "   260,\n",
       "   36,\n",
       "   492,\n",
       "   20,\n",
       "   297,\n",
       "   32,\n",
       "   231,\n",
       "   214,\n",
       "   28,\n",
       "   260,\n",
       "   36,\n",
       "   429,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   227,\n",
       "   190,\n",
       "   32,\n",
       "   260,\n",
       "   39,\n",
       "   456,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   231,\n",
       "   206,\n",
       "   32,\n",
       "   293,\n",
       "   38,\n",
       "   492,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   231,\n",
       "   214,\n",
       "   28,\n",
       "   260,\n",
       "   34,\n",
       "   429,\n",
       "   20,\n",
       "   94,\n",
       "   169,\n",
       "   28,\n",
       "   95,\n",
       "   169,\n",
       "   33,\n",
       "   95,\n",
       "   169,\n",
       "   208,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   38,\n",
       "   291,\n",
       "   44,\n",
       "   264,\n",
       "   210,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   231,\n",
       "   214,\n",
       "   32,\n",
       "   293,\n",
       "   38,\n",
       "   260,\n",
       "   44,\n",
       "   429,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   39,\n",
       "   96,\n",
       "   127,\n",
       "   44,\n",
       "   96,\n",
       "   127,\n",
       "   192,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   39,\n",
       "   93,\n",
       "   127,\n",
       "   196,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   39,\n",
       "   93,\n",
       "   127,\n",
       "   44,\n",
       "   291,\n",
       "   198,\n",
       "   32,\n",
       "   92,\n",
       "   127,\n",
       "   39,\n",
       "   93,\n",
       "   127,\n",
       "   43,\n",
       "   117,\n",
       "   127,\n",
       "   204,\n",
       "   20,\n",
       "   95,\n",
       "   137,\n",
       "   32,\n",
       "   93,\n",
       "   137,\n",
       "   176,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'programs': [[0, False], [0, False]]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_song.numpy()\n",
    "\n",
    "# covert numpy array to list\n",
    "list = sample_song.numpy().tolist()[0]\n",
    "[list]\n",
    "\n",
    "fil = {\"tokens\" : [list,list], \"programs\" : [[0, False],[0, False]]}\n",
    "fil"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63f83f16703f6be7ffd0b8723f3797201bb87b90d9af5c25669f8b533ad7062b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
