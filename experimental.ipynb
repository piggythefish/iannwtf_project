{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import miditok as midi\n",
    "from functions import get_tokens, split_input_target, positional_encoding, masked_loss, masked_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the tokens and some information about our data and some basic prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smallest token:1.0, biggest token:499.0, number of unique tokens:(495,), missing tokens:{0, 178, 180, 182, 186}\n",
      "number of tokens: 1820490\n",
      "number of pieces: 295\n",
      "average tokens per piece: 6171.152542372882\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens, piece_counter = get_tokens() # get all tokens from the dataset\n",
    "unique_tokens, count = np.unique(tokens, return_counts=True)\n",
    "vocab_size = int(np.max(tokens)) + 1 # get the size of the vocabulary\n",
    "\n",
    "print(\"smallest token:{}, biggest token:{}, number of unique tokens:{}, missing tokens:{}\".format(np.min(tokens),np.max(tokens),np.unique(tokens).shape,set(range(0, 500))-set(tokens))) # print some information about the tokens\n",
    "print(\"number of tokens: {}\".format(len(tokens))) # print the number of tokens\n",
    "print(\"number of pieces: {}\".format(piece_counter)) # print the number of pieces\n",
    "print(\"average tokens per piece: {}\".format(len(tokens)/piece_counter)) # print average tokens per piece\n",
    "\n",
    "# plot the distribution of the tokens\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(tokens, bins=500)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = tf.cast(tokens, tf.int32) # put tokens into a tensor and cast to int32\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n] # first 90% of data\n",
    "val_data = data[n:] # last 10% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# postional encoding function\n",
    "def positional_encoding(length, d_model):\n",
    "    pos = np.arange(length)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "pos_encoding = positional_encoding(seq_length, d_model)\n",
    "\n",
    "# plot the positional encoding\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, d_model))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer via subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer for embedding and positional encoding\n",
    "class PostionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, block_size, dropout_rate):\n",
    "        super(PostionalEmbedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.block_size = block_size\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                          output_dim=d_model, # each token gets a 512-vector embedding\n",
    "                          input_length=block_size)\n",
    "        self.pos_encoding = positional_encoding(block_size, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x) + self.pos_encoding[:, :self.block_size, :]\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "# dot-product attention\n",
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.head_size = d_model // n_heads\n",
    "        self.key = tf.keras.layers.Dense(units=self.head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=self.head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=self.head_size, use_bias=False)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute scores\n",
    "        scores = tf.matmul(q,tf.transpose(k, perm=[0, 2, 1])) / tf.math.sqrt(tf.cast(self.head_size, tf.float32))\n",
    "        tril = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
    "        scores = tf.where(tril == 0, tf.fill(tril.shape, -float('inf')), scores)\n",
    "        scores = tf.nn.softmax(scores, axis=-1)\n",
    "        # weighted sum of values\n",
    "        v = self.value(x)\n",
    "        return tf.matmul(scores, v)\n",
    "\n",
    "# multi-head attention\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads,):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = [ScaledDotProductAttention(d_model, n_heads) for _ in range(n_heads)]\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model, use_bias=False)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = tf.concat([head(x) for head in self.heads], axis=-1)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "# feed-forward network\n",
    "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=dff, activation=tf.nn.gelu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "# implementaion of SwiGLUFFN\n",
    "class SwiGLUFFN(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super(SwiGLUFFN, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=(dff//3)*2, activation=tf.nn.silu, use_bias=False)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=(dff//3)*2, use_bias=False)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=d_model, use_bias=False)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        w = self.dense1(x)\n",
    "        v = self.dense2(x)\n",
    "        x = self.dense3(w*v)\n",
    "        return x\n",
    "\n",
    "# decoder layer\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, dff, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = SwiGLUFFN(d_model, dff)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.add = tf.keras.layers.Add()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        res_x = x\n",
    "        x = self.layernorm(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.add([res_x, x])\n",
    "        res_x = x\n",
    "        x = self.layernorm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.add([res_x, x])\n",
    "        return x\n",
    "    \n",
    "# decoder\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, dff, dropout_rate, n_layers, block_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = PostionalEmbedding(vocab_size, d_model, block_size, dropout_rate)\n",
    "        self.layers = [DecoderLayer(d_model, n_heads, dff, dropout_rate) for _ in range(n_layers)]\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "# transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, dff, dropout_rate, n_layers, block_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.decoder = Decoder(vocab_size, d_model, n_heads, dff, dropout_rate, n_layers, block_size)\n",
    "        self.final_dense = tf.keras.layers.Dense(units=vocab_size)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.decoder(x)\n",
    "        logits = self.final_dense(x)\n",
    "            \n",
    "        try:      \n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits\n",
    "    \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 192 # also embedding size\n",
    "dff = 768 # inner feedforward layer dim\n",
    "n_heads = 8 # number of heads in the multihead attention layer\n",
    "d_v, d_q, d_k = d_model // n_heads , d_model // n_heads , d_model // n_heads # dimension of the query, key and value vectors \n",
    "n_layers = 6 # number of layers\n",
    "dropout_rate = 0.1 # dropout rate\n",
    "epochs = 100\n",
    "seq_length = 4096 # length of the sequence\n",
    "batch_size = 32 # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul_1:0\", shape=(None, 4096, 512), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(None, 4096, 512), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(None, 4096, 512), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(None, 4096, 512), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(None, 4096, 512), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(None, 4096, 512), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(None, 4096, 512), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(None, 4096, 512), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(None, 4096, 512), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(None, 4096, 512), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(None, 4096, 512), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(None, 4096, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# create the model with the specified parameters \n",
    "model = Transformer(vocab_size, d_model, n_heads, dff, dropout_rate, n_layers, seq_length)\n",
    "\n",
    "model.build(input_shape=(None, seq_length))\n",
    "\n",
    "# model.load_weights('dmodel 128 dff 512 nheads 8 nlayers 6 dropout 0.1 epochs 100 seqlen 512 batch 64.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4096, 500), dtype=float32, numpy=\n",
       "array([[[ 0.93109655,  1.524544  ,  5.3054953 , ...,  1.8020641 ,\n",
       "          0.5517999 ,  4.177929  ],\n",
       "        [ 1.1230563 ,  1.1251721 ,  4.8477774 , ...,  1.7613504 ,\n",
       "          0.3038156 ,  3.9153748 ],\n",
       "        [ 1.3480986 ,  0.45306554,  4.3155565 , ...,  1.2198333 ,\n",
       "          0.2016418 ,  3.7194157 ],\n",
       "        ...,\n",
       "        [-0.17362699,  0.73992664, -0.7337722 , ..., -0.52947897,\n",
       "          1.1504998 , -2.7702982 ],\n",
       "        [-0.14890286,  0.3638735 , -1.107051  , ..., -0.5768866 ,\n",
       "          1.188055  , -2.6961622 ],\n",
       "        [-0.15275988,  0.02487998, -0.9826255 , ..., -0.6566417 ,\n",
       "          1.1292102 , -2.3084042 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tf.random.uniform((1, seq_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul_1:0\", shape=(1, 512), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(1, 512), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 192), dtype=float32, numpy=\n",
       "array([[-0.30143425, -0.16266592,  0.13922173,  0.38900506, -0.3960744 ,\n",
       "         0.24347688, -0.1372642 , -0.00229743,  0.15368676,  0.71757644,\n",
       "        -0.23209517,  0.3859207 ,  0.03606668, -0.538191  ,  0.0056992 ,\n",
       "         0.41539985, -0.01221757, -0.37797865, -0.01549714, -0.09799986,\n",
       "        -0.20575377,  0.29777956, -0.06646045, -0.13448471, -0.31992278,\n",
       "         0.3548094 , -0.16118677, -0.2725544 ,  0.5411022 , -0.3572122 ,\n",
       "        -0.28140196,  0.5274282 ,  0.03013147,  0.6052353 , -0.2469005 ,\n",
       "         0.48191392,  0.2865448 ,  0.06642406, -0.1946268 ,  0.12518826,\n",
       "         0.29608214,  0.5940161 ,  0.41078615, -0.0652066 ,  0.59065354,\n",
       "         0.2323617 , -0.24860588, -0.20385401,  0.19536783,  0.5349349 ,\n",
       "         0.1904856 , -0.59692186, -0.1420533 , -0.03597724, -0.19153205,\n",
       "        -0.43292904, -0.14504431,  0.4378357 , -0.631899  , -0.2034197 ,\n",
       "        -0.70258385,  0.28495514, -0.0980965 , -0.4907956 , -0.17711607,\n",
       "         0.11481193,  0.8001166 ,  0.05917355, -0.2065489 , -0.5895711 ,\n",
       "        -0.600058  ,  0.04771294, -0.21977356,  0.45111215,  0.074457  ,\n",
       "        -0.819276  ,  0.22390136,  0.0720375 , -0.18036996, -0.29748955,\n",
       "        -0.20707464,  0.3483567 , -0.3000375 ,  0.06039674, -0.5788627 ,\n",
       "         1.0389576 ,  0.80388755,  0.27108493, -0.4343495 ,  0.5366455 ,\n",
       "        -0.2959247 , -0.15204863, -0.4746924 , -0.53769755,  0.73617035,\n",
       "         0.37166208, -0.12314501, -0.35640028,  0.00724006, -0.3003128 ,\n",
       "         0.23287722,  0.19989425, -0.15668514,  0.9117515 ,  0.40925437,\n",
       "        -0.36536637, -0.08879621,  0.36167347,  0.14581615,  0.05915128,\n",
       "         0.32403928, -0.30144963, -0.39406314,  0.12170674, -0.543212  ,\n",
       "        -0.5453219 ,  0.16578695, -0.15717703,  0.13468495, -0.19517435,\n",
       "         0.27315074,  0.2837275 , -0.39665404, -0.35683963, -0.5849543 ,\n",
       "        -0.57145745, -0.68064845, -0.41497198, -0.65529877, -0.22867645,\n",
       "         0.20006204,  0.09680691, -0.26517576, -0.36843345, -0.02499005,\n",
       "         0.7841175 ,  0.09026684, -0.22447084,  0.45620012, -0.20250206,\n",
       "         0.05070694, -0.26441833, -0.5168284 , -0.47258106, -0.7256201 ,\n",
       "         0.43145522,  0.37452903,  0.202008  , -0.13196321, -0.13250946,\n",
       "         0.01626722,  0.1213524 ,  0.49107584, -0.2566421 , -0.42416352,\n",
       "        -0.09730526,  0.41173083,  0.3711897 ,  0.1259216 , -0.50125426,\n",
       "        -0.17649782,  0.24116473,  0.5382097 , -0.33558154,  0.14108373,\n",
       "         0.4909855 ,  0.02524684,  0.32868776, -0.06012473,  0.19323684,\n",
       "         0.00304746,  0.01672539, -0.20742254, -0.4100137 , -0.06486969,\n",
       "        -0.6353241 ,  0.21882565, -0.0326341 , -0.02471158,  0.35192394,\n",
       "        -0.0187725 ,  0.6364086 , -0.71053094, -0.00934828, -0.44594246,\n",
       "        -0.24574919, -0.5230757 , -0.9661106 , -0.2620933 ,  0.18946663,\n",
       "        -0.17935488,  0.06326337]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = SwiGLUFFN(d_model, dff)\n",
    "l(tf.random.uniform((1, seq_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf.data.Dataset objects for training and validation\n",
    "train_ds, val_ds = tf.data.Dataset.from_tensor_slices(train_data), tf.data.Dataset.from_tensor_slices(val_data)\n",
    "# generate sequences of length with .batch and then split the sequences into input and target\n",
    "train_ds, val_ds = train_ds.batch(seq_length+1, drop_remainder=True).map(split_input_target), val_ds.batch(seq_length+1, drop_remainder=True).map(split_input_target) \n",
    "# shuffle, batch and prefetch as usual\n",
    "train_ds, val_ds = make_batches(train_ds), make_batches(val_ds)\n",
    "\n",
    "# set the learning rate to our creeted custom schedule\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "# create the optimizer with the learning rate and the other parameters as specified in the paper\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "# create the model with the specified parameters \n",
    "model = Transformer(vocab_size, d_model, n_heads, dff, dropout_rate, n_layers, seq_length)\n",
    "\n",
    "model.build(input_shape=(None, seq_length))\n",
    "\n",
    "model.load_weights('dmodel 128 dff 512 nheads 8 nlayers 6 dropout 0.1 epochs 100 seqlen 512 batch 64.h5')\n",
    "\n",
    "# save weights of the model after every epoch\n",
    "checkpoint_path = \"checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(transformer=model,\n",
    "                            optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=None)\n",
    "\n",
    "# compile the model with the masked loss and masked accuracy\n",
    "model.compile(optimizer=optimizer, loss=masked_loss, metrics=[masked_accuracy])\n",
    "\n",
    "# train the model \n",
    "model.fit(train_ds, epochs=epochs, validation_data=val_ds)\n",
    "\n",
    "# save the model\n",
    "model.save_weights('dmodel:{} dff:{} nheads:{} nlayers:{} dropout:{} epochs:{} seqlen:{} batch:{}.h5'.format(d_model, dff, n_heads, n_layers, dropout_rate, epochs, seq_length, batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 512\n",
    "start_token = tf.constant([[89] + [3] + [0] * (seq_length)], dtype=tf.int64)\n",
    "np.delete(start_token, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 511\n",
    "start_token = tf.constant([[89] + [0] * (seq_length - 1)], dtype=tf.int64)\n",
    "\n",
    "def generate_tokens_greedy(model, start_token, max_len):\n",
    "    for i in range(max_len):\n",
    "        logits = model(start_token) # generate logits for the next token\n",
    "        probs = tf.nn.softmax(logits, axis = -1) # get the probabilities \n",
    "        argmax = tf.argmax(probs, axis = -1) # get the token with the highest probability\n",
    "        start_token_numpy = start_token.numpy() # convert the tensor to numpy array\n",
    "        start_token_numpy[:, i+1] = argmax[:,i].numpy() # add the predicted token to the sequence\n",
    "        start_token = tf.constant(start_token_numpy, dtype=tf.int64) # convert the numpy array to tensor\n",
    "        if argmax[:,i].numpy() == 176: # stop when the end token is predicted\n",
    "            break\n",
    "    return start_token\n",
    "\n",
    "def generate_tokens_sampling(model, start_token, max_len):\n",
    "    for i in range(max_len):\n",
    "        logits = model(start_token) # generate logits for the next token\n",
    "        start_token_numpy = start_token.numpy() # convert the tensor to numpy array\n",
    "        start_token_numpy[:, i+1] = tf.random.categorical(logits[:,i], num_samples=1).numpy() # sample a token from the logits and add it to the sequence\n",
    "        start_token = tf.constant(start_token_numpy, dtype=tf.int64) # convert the numpy array to tensor\n",
    "        if start_token_numpy[:, i+1] == 176: # stop when the end token is predicted\n",
    "            break\n",
    "    return start_token\n",
    "\n",
    "sample_song = generate_tokens_sampling(model, start_token, max_len)\n",
    "sample_song.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import REMI, MIDITokenizer\n",
    "tokenizer = REMI()\n",
    "tokenizer.load_params('Dataset_tokenized_BPE/config.txt')\n",
    "coverted = tokenizer(sample_song, [(0, False)])\n",
    "coverted.dump('sample_song_sampling.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import REMI, MIDITokenizer\n",
    "from miditok.utils import get_midi_programs\n",
    "from miditoolkit import MidiFile\n",
    "from pathlib import Path\n",
    "tokenizer = REMI()\n",
    "cock = MidiFile('Final_Project/Dataset/chopin/chp_op18.mid')\n",
    "schwanz = tokenizer(MidiFile('Final_Project/Dataset/chopin/chp_op31.mid'))\n",
    "tokens = tokenizer(cock) \n",
    "coverted_back = tokenizer(tokens, [(0, False), (0, False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def json_rdy(sample):\n",
    "    sample = sample.numpy().tolist()[0]\n",
    "    sample = {\"tokens\" : [sample,sample], \"programs\" : [[0, False],[0, False]]}\n",
    "    with open('{}.json', 'w'.format(sample)) as fp:\n",
    "        json.dump(json_rdy(sample), fp)    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [[89,\n",
       "   218,\n",
       "   16,\n",
       "   96,\n",
       "   153,\n",
       "   28,\n",
       "   97,\n",
       "   153,\n",
       "   194,\n",
       "   28,\n",
       "   96,\n",
       "   252,\n",
       "   32,\n",
       "   97,\n",
       "   248,\n",
       "   21,\n",
       "   94,\n",
       "   157,\n",
       "   28,\n",
       "   96,\n",
       "   157,\n",
       "   33,\n",
       "   97,\n",
       "   153,\n",
       "   1,\n",
       "   192,\n",
       "   28,\n",
       "   96,\n",
       "   139,\n",
       "   193,\n",
       "   28,\n",
       "   96,\n",
       "   138,\n",
       "   35,\n",
       "   96,\n",
       "   138,\n",
       "   36,\n",
       "   97,\n",
       "   138,\n",
       "   194,\n",
       "   16,\n",
       "   258,\n",
       "   28,\n",
       "   231,\n",
       "   198,\n",
       "   28,\n",
       "   456,\n",
       "   40,\n",
       "   260,\n",
       "   218,\n",
       "   16,\n",
       "   258,\n",
       "   28,\n",
       "   231,\n",
       "   190,\n",
       "   28,\n",
       "   260,\n",
       "   36,\n",
       "   456,\n",
       "   16,\n",
       "   258,\n",
       "   28,\n",
       "   231,\n",
       "   206,\n",
       "   28,\n",
       "   260,\n",
       "   36,\n",
       "   492,\n",
       "   20,\n",
       "   297,\n",
       "   32,\n",
       "   231,\n",
       "   214,\n",
       "   28,\n",
       "   260,\n",
       "   36,\n",
       "   429,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   227,\n",
       "   190,\n",
       "   32,\n",
       "   260,\n",
       "   39,\n",
       "   456,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   231,\n",
       "   206,\n",
       "   32,\n",
       "   293,\n",
       "   38,\n",
       "   492,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   231,\n",
       "   214,\n",
       "   28,\n",
       "   260,\n",
       "   34,\n",
       "   429,\n",
       "   20,\n",
       "   94,\n",
       "   169,\n",
       "   28,\n",
       "   95,\n",
       "   169,\n",
       "   33,\n",
       "   95,\n",
       "   169,\n",
       "   208,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   38,\n",
       "   291,\n",
       "   44,\n",
       "   264,\n",
       "   210,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   231,\n",
       "   214,\n",
       "   32,\n",
       "   293,\n",
       "   38,\n",
       "   260,\n",
       "   44,\n",
       "   429,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   39,\n",
       "   96,\n",
       "   127,\n",
       "   44,\n",
       "   96,\n",
       "   127,\n",
       "   192,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   39,\n",
       "   93,\n",
       "   127,\n",
       "   196,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   39,\n",
       "   93,\n",
       "   127,\n",
       "   44,\n",
       "   291,\n",
       "   198,\n",
       "   32,\n",
       "   92,\n",
       "   127,\n",
       "   39,\n",
       "   93,\n",
       "   127,\n",
       "   43,\n",
       "   117,\n",
       "   127,\n",
       "   204,\n",
       "   20,\n",
       "   95,\n",
       "   137,\n",
       "   32,\n",
       "   93,\n",
       "   137,\n",
       "   176,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [89,\n",
       "   218,\n",
       "   16,\n",
       "   96,\n",
       "   153,\n",
       "   28,\n",
       "   97,\n",
       "   153,\n",
       "   194,\n",
       "   28,\n",
       "   96,\n",
       "   252,\n",
       "   32,\n",
       "   97,\n",
       "   248,\n",
       "   21,\n",
       "   94,\n",
       "   157,\n",
       "   28,\n",
       "   96,\n",
       "   157,\n",
       "   33,\n",
       "   97,\n",
       "   153,\n",
       "   1,\n",
       "   192,\n",
       "   28,\n",
       "   96,\n",
       "   139,\n",
       "   193,\n",
       "   28,\n",
       "   96,\n",
       "   138,\n",
       "   35,\n",
       "   96,\n",
       "   138,\n",
       "   36,\n",
       "   97,\n",
       "   138,\n",
       "   194,\n",
       "   16,\n",
       "   258,\n",
       "   28,\n",
       "   231,\n",
       "   198,\n",
       "   28,\n",
       "   456,\n",
       "   40,\n",
       "   260,\n",
       "   218,\n",
       "   16,\n",
       "   258,\n",
       "   28,\n",
       "   231,\n",
       "   190,\n",
       "   28,\n",
       "   260,\n",
       "   36,\n",
       "   456,\n",
       "   16,\n",
       "   258,\n",
       "   28,\n",
       "   231,\n",
       "   206,\n",
       "   28,\n",
       "   260,\n",
       "   36,\n",
       "   492,\n",
       "   20,\n",
       "   297,\n",
       "   32,\n",
       "   231,\n",
       "   214,\n",
       "   28,\n",
       "   260,\n",
       "   36,\n",
       "   429,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   227,\n",
       "   190,\n",
       "   32,\n",
       "   260,\n",
       "   39,\n",
       "   456,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   231,\n",
       "   206,\n",
       "   32,\n",
       "   293,\n",
       "   38,\n",
       "   492,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   231,\n",
       "   214,\n",
       "   28,\n",
       "   260,\n",
       "   34,\n",
       "   429,\n",
       "   20,\n",
       "   94,\n",
       "   169,\n",
       "   28,\n",
       "   95,\n",
       "   169,\n",
       "   33,\n",
       "   95,\n",
       "   169,\n",
       "   208,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   38,\n",
       "   291,\n",
       "   44,\n",
       "   264,\n",
       "   210,\n",
       "   20,\n",
       "   258,\n",
       "   32,\n",
       "   231,\n",
       "   214,\n",
       "   32,\n",
       "   293,\n",
       "   38,\n",
       "   260,\n",
       "   44,\n",
       "   429,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   39,\n",
       "   96,\n",
       "   127,\n",
       "   44,\n",
       "   96,\n",
       "   127,\n",
       "   192,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   39,\n",
       "   93,\n",
       "   127,\n",
       "   196,\n",
       "   32,\n",
       "   93,\n",
       "   127,\n",
       "   39,\n",
       "   93,\n",
       "   127,\n",
       "   44,\n",
       "   291,\n",
       "   198,\n",
       "   32,\n",
       "   92,\n",
       "   127,\n",
       "   39,\n",
       "   93,\n",
       "   127,\n",
       "   43,\n",
       "   117,\n",
       "   127,\n",
       "   204,\n",
       "   20,\n",
       "   95,\n",
       "   137,\n",
       "   32,\n",
       "   93,\n",
       "   137,\n",
       "   176,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'programs': [[0, False], [0, False]]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_song.numpy()\n",
    "\n",
    "# covert numpy array to list\n",
    "list = sample_song.numpy().tolist()[0]\n",
    "[list]\n",
    "\n",
    "fil = {\"tokens\" : [list,list], \"programs\" : [[0, False],[0, False]]}\n",
    "fil"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63f83f16703f6be7ffd0b8723f3797201bb87b90d9af5c25669f8b533ad7062b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
