{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns all the json files in a folder into numpy arrays\n",
    "def json_to_nparray():\n",
    "    for filename in os.listdir('Dataset_tokenized_BPE'):\n",
    "        if filename.endswith('.json'):\n",
    "            with open('Dataset_tokenized_BPE/' + filename) as f:\n",
    "                data = json.load(f)\n",
    "                np.save('Dataset_tokenized_BPE/' + filename[:-5], np.array(data))\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "# revert the above function to get the json files back from the numpy arrays\n",
    "def nparray_to_json():\n",
    "    for filename in os.listdir('Dataset_tokenized_BPE'):\n",
    "        if filename.endswith('.npy'):\n",
    "            data = np.load('Dataset_tokenized_BPE/' + filename)\n",
    "            with open('Dataset_tokenized_BPE/' + filename[:-4] + '.json', 'w') as f:\n",
    "                json.dump(data.tolist(), f)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "# open .npy files from a folder and concatenate them into one numpy array\n",
    "def npy_to_nparray():   \n",
    "    data = []\n",
    "    for filename in os.listdir('Dataset_tokenized_BPE'):\n",
    "        if filename.endswith('.npy'):\n",
    "            data.append(np.load('Dataset_tokenized_BPE/' + filename))\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open .npy files from a folder and concatenate all the tokens into one numpy array\n",
    "tokens = np.array(()) # empty numpy array\n",
    "for filename in os.listdir('Dataset_tokenized_BPE'): # iterate through all files in the folder\n",
    "    if filename.endswith('.npy'): # if the file is a .npy file\n",
    "        for i in range(len(np.load(('Dataset_tokenized_BPE/' + filename), allow_pickle=True)[()].get('tokens'))): # iterate through all the tokens in the file\n",
    "            tokens = np.append(tokens, np.load(('Dataset_tokenized_BPE/' + filename), allow_pickle=True)[()].get('tokens')[i]) # append the tokens to the numpy array\n",
    "        continue \n",
    "    else: \n",
    "        continue \n",
    "    \n",
    "# revert the above funtion to get the .npy files back from the numpy array\n",
    "def nparray_to_npy():\n",
    "    # create a folder to store the .npy files\n",
    "    if not os.path.exists('Dataset_tokenized_BPE'):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1819900,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x = train_data[:BLOCK_SIZE]\n",
    "y = train_data[1:BLOCK_SIZE+1]\n",
    "for t in range(BLOCK_SIZE):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")\n",
    "\n",
    "xb , yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('----')\n",
    "\n",
    "\n",
    "for b in range(BATCH_SIZE): # batch dimension\n",
    "    for t in range(BLOCK_SIZE): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.numpy().tolist()} the target: {target}\")#\n",
    "\"\"\"\n",
    "\n",
    "data = tf.cast(tokens, tf.int32) # put tokens into a tensor and cast to int32\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n] # first 90% of data\n",
    "val_data = data[n:] # last 10% of data\n",
    "\n",
    "BLOCK_SIZE = 12\n",
    "BATCH_SIZE = 2\n",
    "    \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = tf.random.uniform((BATCH_SIZE,), maxval=len(data)-BLOCK_SIZE, dtype=tf.int32)\n",
    "    x = tf.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = tf.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(500, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = data[np.argmax(tokens)] + 1\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(pt, en): \n",
    "    pt = tokenizers.pt.tokenize(pt)      # Output is ragged.\n",
    "    pt = pt[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
    "    pt = pt.to_tensor()  # Convert to 0-padded dense Tensor\n",
    "\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    en = en[:, :(MAX_TOKENS+1)]\n",
    "    en_inputs = en[:, :-1].to_tensor()  # Drop the [END] tokens\n",
    "    en_labels = en[:, 1:].to_tensor()   # Drop the [START] tokens\n",
    "\n",
    "    return (pt, en_inputs), en_labels\n",
    "\n",
    " \n",
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "# positional encoding function \n",
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = tf.range(length,dtype=tf.float32)[:, None]     # (seq, 1)\n",
    "  depths = tf.range(depth,dtype=tf.float32)[None, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = tf.concat(\n",
    "      [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "  return pos_encoding\n",
    "\n",
    "# positional embedding layer \n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x\n",
    "  \n",
    "l = PositionalEmbedding(1000,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 16, 512) dtype=float32 (created by layer 'positional_embedding')>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(tf.keras.Input(shape=(16,),dtype = tf.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.causal_self_attention(x)\n",
    "    x = self.cross_attention(x,x)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    x  = inputs\n",
    "\n",
    "    x = self.decoder(x)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "MAX_TOKENS = 10\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=vocab_size,\n",
    "    target_vocab_size=vocab_size,\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a ,b = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ds_gen():\n",
    "    for i in range(1000):\n",
    "        yield get_batch('train')\n",
    "        \n",
    "def val_ds_gen():\n",
    "    for i in range(100):\n",
    "        yield get_batch('train')\n",
    "        \n",
    "train_ds= tf.data.Dataset.from_generator(train_ds_gen, output_types=(tf.int32, tf.int32))\n",
    "val_ds = tf.data.Dataset.from_generator(val_ds_gen, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 16, 256) dtype=float32 (created by layer 'multi_head_attention_8')>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.keras.Input(shape=(16,256))\n",
    "l = tf.keras.layers.MultiHeadAttention(num_heads=8, key_dim=64)(a, a,a,use_causal_mask = True)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 12, 500), dtype=float32, numpy=\n",
       "array([[[ 0.41843367, -0.6640315 , -0.9288635 , ..., -0.18519644,\n",
       "         -0.21592776, -0.15706   ],\n",
       "        [ 0.20486599, -1.1446135 , -0.23492339, ..., -0.14431347,\n",
       "         -0.05127469, -0.03966749],\n",
       "        [ 0.28998542, -0.7843472 , -0.4523071 , ...,  0.45857686,\n",
       "         -1.1024239 ,  0.01032036],\n",
       "        ...,\n",
       "        [-0.04622516, -0.1424579 ,  0.12012563, ...,  1.1061985 ,\n",
       "         -1.536612  ,  0.40399054],\n",
       "        [ 0.49052036, -0.7470296 ,  0.11027565, ...,  0.2995183 ,\n",
       "         -0.22417507,  0.4130093 ],\n",
       "        [-0.09576005,  0.3553278 ,  0.04561641, ...,  0.7936692 ,\n",
       "         -1.3002354 ,  0.47641712]],\n",
       "\n",
       "       [[ 0.47951475, -0.93732077, -0.5168369 , ..., -0.3730178 ,\n",
       "         -0.20323567, -0.05597189],\n",
       "        [ 0.9160687 , -1.0253688 , -0.22526588, ..., -0.2322132 ,\n",
       "          0.1306046 , -0.13766426],\n",
       "        [ 0.56257695, -0.96330166, -0.6565111 , ...,  0.59725106,\n",
       "         -0.0322721 ,  0.01379591],\n",
       "        ...,\n",
       "        [ 0.60275555, -1.5671514 ,  0.4065893 , ...,  0.32707375,\n",
       "         -0.8321703 ,  0.968511  ],\n",
       "        [ 0.37922186, -1.286062  ,  0.2531468 , ..., -0.10216659,\n",
       "         -0.3641791 ,  0.44145805],\n",
       "        [ 0.79164565,  0.02236928,  0.01053597, ...,  0.29258296,\n",
       "         -0.22538453,  0.09531519]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(get_batch('train')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_file4jgx6f_3.py\", line 11, in tf__call\n        x = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(x),), None, fscope)\n    File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filecd3uufp5.py\", line 25, in tf__call\n        ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.ld(self).num_layers,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'i'})\n    File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filecd3uufp5.py\", line 23, in loop_body\n        x = ag__.converted_call(ag__.ld(self).dec_layers[ag__.ld(i)], (ag__.ld(x),), None, fscope)\n    File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filea7949dzj.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).causal_self_attention, (ag__.ld(x),), None, fscope)\n    File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filedl1p_gdt.py\", line 10, in tf__call\n        attn_output = ag__.converted_call(ag__.ld(self).mha, (), dict(query=ag__.ld(x), value=ag__.ld(x), key=ag__.ld(x), use_causal_mask=True), fscope)\n\n    ValueError: Exception encountered when calling layer \"transformer\" \"                 f\"(type Transformer).\n    \n    in user code:\n    \n        File \"C:\\Users\\benja\\AppData\\Local\\Temp\\ipykernel_17576\\2261851629.py\", line 188, in call  *\n            x = self.decoder(x)  # (batch_size, target_len, d_model)\n        File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filecd3uufp5.py\", line 25, in tf__call\n            ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.ld(self).num_layers,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'i'})\n        File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filecd3uufp5.py\", line 23, in loop_body\n            x = ag__.converted_call(ag__.ld(self).dec_layers[ag__.ld(i)], (ag__.ld(x),), None, fscope)\n        File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filea7949dzj.py\", line 10, in tf__call\n            x = ag__.converted_call(ag__.ld(self).causal_self_attention, (ag__.ld(x),), None, fscope)\n        File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filedl1p_gdt.py\", line 10, in tf__call\n            attn_output = ag__.converted_call(ag__.ld(self).mha, (), dict(query=ag__.ld(x), value=ag__.ld(x), key=ag__.ld(x), use_causal_mask=True), fscope)\n    \n        ValueError: Exception encountered when calling layer \"decoder\" \"                 f\"(type Decoder).\n        \n        in user code:\n        \n            File \"C:\\Users\\benja\\AppData\\Local\\Temp\\ipykernel_17576\\2261851629.py\", line 164, in call  *\n                x  = self.dec_layers[i](x)\n            File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filea7949dzj.py\", line 10, in tf__call\n                x = ag__.converted_call(ag__.ld(self).causal_self_attention, (ag__.ld(x),), None, fscope)\n            File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filedl1p_gdt.py\", line 10, in tf__call\n                attn_output = ag__.converted_call(ag__.ld(self).mha, (), dict(query=ag__.ld(x), value=ag__.ld(x), key=ag__.ld(x), use_causal_mask=True), fscope)\n        \n            ValueError: Exception encountered when calling layer \"decoder_layer\" \"                 f\"(type DecoderLayer).\n            \n            in user code:\n            \n                File \"C:\\Users\\benja\\AppData\\Local\\Temp\\ipykernel_17576\\2261851629.py\", line 130, in call  *\n                    x = self.causal_self_attention(x)\n                File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filedl1p_gdt.py\", line 10, in tf__call\n                    attn_output = ag__.converted_call(ag__.ld(self).mha, (), dict(query=ag__.ld(x), value=ag__.ld(x), key=ag__.ld(x), use_causal_mask=True), fscope)\n            \n                ValueError: Exception encountered when calling layer \"causal_self_attention\" \"                 f\"(type CausalSelfAttention).\n                \n                in user code:\n                \n                    File \"C:\\Users\\benja\\AppData\\Local\\Temp\\ipykernel_17576\\2261851629.py\", line 36, in call  *\n                        attn_output = self.mha(\n                    File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                \n                    ValueError: Exception encountered when calling layer \"multi_head_attention\" \"                 f\"(type MultiHeadAttention).\n                    \n                    Cannot take the length of shape with unknown rank.\n                    \n                    Call arguments received by layer \"multi_head_attention\" \"                 f\"(type MultiHeadAttention):\n                      • query=tf.Tensor(shape=<unknown>, dtype=float32)\n                      • value=tf.Tensor(shape=<unknown>, dtype=float32)\n                      • key=tf.Tensor(shape=<unknown>, dtype=float32)\n                      • attention_mask=None\n                      • return_attention_scores=False\n                      • training=True\n                      • use_causal_mask=True\n                \n                \n                Call arguments received by layer \"causal_self_attention\" \"                 f\"(type CausalSelfAttention):\n                  • x=tf.Tensor(shape=<unknown>, dtype=float32)\n            \n            \n            Call arguments received by layer \"decoder_layer\" \"                 f\"(type DecoderLayer):\n              • x=tf.Tensor(shape=<unknown>, dtype=float32)\n        \n        \n        Call arguments received by layer \"decoder\" \"                 f\"(type Decoder):\n          • x=tf.Tensor(shape=<unknown>, dtype=int32)\n    \n    \n    Call arguments received by layer \"transformer\" \"                 f\"(type Transformer):\n      • inputs=tf.Tensor(shape=<unknown>, dtype=int32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m transformer\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m      2\u001b[0m     loss\u001b[39m=\u001b[39mmasked_loss,\n\u001b[0;32m      3\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m      4\u001b[0m     metrics\u001b[39m=\u001b[39m[masked_accuracy])\n\u001b[1;32m----> 6\u001b[0m transformer\u001b[39m.\u001b[39;49mfit(train_ds,\n\u001b[0;32m      7\u001b[0m                 epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m                 validation_data\u001b[39m=\u001b[39;49mval_ds)\n",
      "File \u001b[1;32mc:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileid5pvxyx.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file4jgx6f_3.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     10\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(inputs)\n\u001b[1;32m---> 11\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mdecoder, (ag__\u001b[39m.\u001b[39;49mld(x),), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[0;32m     12\u001b[0m logits \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfinal_layer, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filecd3uufp5.py:25\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m     x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdec_layers[ag__\u001b[39m.\u001b[39mld(i)], (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     24\u001b[0m i \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m ag__\u001b[39m.\u001b[39;49mfor_stmt(ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mrange\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mnum_layers,), \u001b[39mNone\u001b[39;49;00m, fscope), \u001b[39mNone\u001b[39;49;00m, loop_body, get_state, set_state, (\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m,), {\u001b[39m'\u001b[39;49m\u001b[39miterate_names\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m'\u001b[39;49m})\n\u001b[0;32m     26\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mlast_attn_scores \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdec_layers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mlast_attn_scores\n\u001b[0;32m     27\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filecd3uufp5.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body\u001b[1;34m(itr)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mnonlocal\u001b[39;00m x\n\u001b[0;32m     22\u001b[0m i \u001b[39m=\u001b[39m itr\n\u001b[1;32m---> 23\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mdec_layers[ag__\u001b[39m.\u001b[39;49mld(i)], (ag__\u001b[39m.\u001b[39;49mld(x),), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filea7949dzj.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcausal_self_attention, (ag__\u001b[39m.\u001b[39;49mld(x),), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[0;32m     11\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcross_attention, (ag__\u001b[39m.\u001b[39mld(x), ag__\u001b[39m.\u001b[39mld(x)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mlast_attn_scores \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcross_attention\u001b[39m.\u001b[39mlast_attn_scores\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filedl1p_gdt.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m attn_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mmha, (), \u001b[39mdict\u001b[39;49m(query\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(x), value\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(x), key\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(x), use_causal_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), fscope)\n\u001b[0;32m     11\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39madd, ([ag__\u001b[39m.\u001b[39mld(x), ag__\u001b[39m.\u001b[39mld(attn_output)],), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mlayernorm, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_file4jgx6f_3.py\", line 11, in tf__call\n        x = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(x),), None, fscope)\n    File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filecd3uufp5.py\", line 25, in tf__call\n        ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.ld(self).num_layers,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'i'})\n    File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filecd3uufp5.py\", line 23, in loop_body\n        x = ag__.converted_call(ag__.ld(self).dec_layers[ag__.ld(i)], (ag__.ld(x),), None, fscope)\n    File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filea7949dzj.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).causal_self_attention, (ag__.ld(x),), None, fscope)\n    File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filedl1p_gdt.py\", line 10, in tf__call\n        attn_output = ag__.converted_call(ag__.ld(self).mha, (), dict(query=ag__.ld(x), value=ag__.ld(x), key=ag__.ld(x), use_causal_mask=True), fscope)\n\n    ValueError: Exception encountered when calling layer \"transformer\" \"                 f\"(type Transformer).\n    \n    in user code:\n    \n        File \"C:\\Users\\benja\\AppData\\Local\\Temp\\ipykernel_17576\\2261851629.py\", line 188, in call  *\n            x = self.decoder(x)  # (batch_size, target_len, d_model)\n        File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filecd3uufp5.py\", line 25, in tf__call\n            ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.ld(self).num_layers,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'i'})\n        File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filecd3uufp5.py\", line 23, in loop_body\n            x = ag__.converted_call(ag__.ld(self).dec_layers[ag__.ld(i)], (ag__.ld(x),), None, fscope)\n        File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filea7949dzj.py\", line 10, in tf__call\n            x = ag__.converted_call(ag__.ld(self).causal_self_attention, (ag__.ld(x),), None, fscope)\n        File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filedl1p_gdt.py\", line 10, in tf__call\n            attn_output = ag__.converted_call(ag__.ld(self).mha, (), dict(query=ag__.ld(x), value=ag__.ld(x), key=ag__.ld(x), use_causal_mask=True), fscope)\n    \n        ValueError: Exception encountered when calling layer \"decoder\" \"                 f\"(type Decoder).\n        \n        in user code:\n        \n            File \"C:\\Users\\benja\\AppData\\Local\\Temp\\ipykernel_17576\\2261851629.py\", line 164, in call  *\n                x  = self.dec_layers[i](x)\n            File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filea7949dzj.py\", line 10, in tf__call\n                x = ag__.converted_call(ag__.ld(self).causal_self_attention, (ag__.ld(x),), None, fscope)\n            File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filedl1p_gdt.py\", line 10, in tf__call\n                attn_output = ag__.converted_call(ag__.ld(self).mha, (), dict(query=ag__.ld(x), value=ag__.ld(x), key=ag__.ld(x), use_causal_mask=True), fscope)\n        \n            ValueError: Exception encountered when calling layer \"decoder_layer\" \"                 f\"(type DecoderLayer).\n            \n            in user code:\n            \n                File \"C:\\Users\\benja\\AppData\\Local\\Temp\\ipykernel_17576\\2261851629.py\", line 130, in call  *\n                    x = self.causal_self_attention(x)\n                File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"C:\\Users\\benja\\AppData\\Local\\Temp\\__autograph_generated_filedl1p_gdt.py\", line 10, in tf__call\n                    attn_output = ag__.converted_call(ag__.ld(self).mha, (), dict(query=ag__.ld(x), value=ag__.ld(x), key=ag__.ld(x), use_causal_mask=True), fscope)\n            \n                ValueError: Exception encountered when calling layer \"causal_self_attention\" \"                 f\"(type CausalSelfAttention).\n                \n                in user code:\n                \n                    File \"C:\\Users\\benja\\AppData\\Local\\Temp\\ipykernel_17576\\2261851629.py\", line 36, in call  *\n                        attn_output = self.mha(\n                    File \"c:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                \n                    ValueError: Exception encountered when calling layer \"multi_head_attention\" \"                 f\"(type MultiHeadAttention).\n                    \n                    Cannot take the length of shape with unknown rank.\n                    \n                    Call arguments received by layer \"multi_head_attention\" \"                 f\"(type MultiHeadAttention):\n                      • query=tf.Tensor(shape=<unknown>, dtype=float32)\n                      • value=tf.Tensor(shape=<unknown>, dtype=float32)\n                      • key=tf.Tensor(shape=<unknown>, dtype=float32)\n                      • attention_mask=None\n                      • return_attention_scores=False\n                      • training=True\n                      • use_causal_mask=True\n                \n                \n                Call arguments received by layer \"causal_self_attention\" \"                 f\"(type CausalSelfAttention):\n                  • x=tf.Tensor(shape=<unknown>, dtype=float32)\n            \n            \n            Call arguments received by layer \"decoder_layer\" \"                 f\"(type DecoderLayer):\n              • x=tf.Tensor(shape=<unknown>, dtype=float32)\n        \n        \n        Call arguments received by layer \"decoder\" \"                 f\"(type Decoder):\n          • x=tf.Tensor(shape=<unknown>, dtype=int32)\n    \n    \n    Call arguments received by layer \"transformer\" \"                 f\"(type Transformer):\n      • inputs=tf.Tensor(shape=<unknown>, dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])\n",
    "\n",
    "transformer.fit(train_ds,\n",
    "                epochs=20,\n",
    "                validation_data=val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63f83f16703f6be7ffd0b8723f3797201bb87b90d9af5c25669f8b533ad7062b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
